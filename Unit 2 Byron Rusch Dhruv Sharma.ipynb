{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unit 2 Byron Rusch Dhruv Sharma.ipynb","provenance":[],"authorship_tag":"ABX9TyPF14nVnJtoB9I8UsdvzJSs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M4LC-dd3VTaQ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#Defining F\n","z = (1/(2**0.5)) * np.transpose(np.array([2,1]))\n","\n","dFdx1 = lambda x: x[0]/2\n","dFdx2 = lambda x: 2*x[1]\n","\n","F1Gradz = np.array([dFdx1(z), dFdx2(z)])\n","F1GradNormz = np.linalg.norm(F1Gradz)\n","\n","n = F1Gradz/F1GradNormz\n","t = np.array([n[1],-n[0]])\n","\n","F1 = lambda x: (x[0]/2)**2 + x[1]**2 - 1\n","F2 = lambda x,p: np.dot(n,(x-z)) - p - (1/2) * (np.dot(t,(x-z)))**2\n","\n","dF2 = lambda x: n - t*(np.dot(t,(x-z)))\n","\n","#Jacobian = [[dF1/dx1, dF1/dx2], [dF2/dx1, dF2/dx2]]\n","Jacobian = lambda x: np.vstack([np.array([dFdx1(x), dFdx2(x)]), np.array([dF2(x)])])\n","\n","initialGuessPlus = z + 2*(n+t)\n","initialGuessMinus = z + 2*(n-t)"]},{"cell_type":"code","source":["#PART-(a) with p=0.5\n","p_ = -0.5\n","F = lambda x: np.transpose(np.array([F1(x), F2(x,p_)]))\n","plotResidual_partB(p_)\n","\n","#First initial Guess\n","xPlus,successPlus,errorPlus,xHistPlus = newton(F,Jacobian,initialGuessPlus)\n","\n","#separating the vector values into two arrays for x1 and x2\n","resultTop = []\n","resultBottom = []\n","for result in xHistPlus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xPlus',c='r')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xPlus',c='g')\n","\n","print(f'Does the Newton Method converge? : Success = {successPlus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","#Second initial guess\n","xMinus,successMinus,errorMinus,xHistMinus = newton(F,Jacobian,initialGuessMinus)\n","\n","resultTop = []\n","resultBottom = []\n","for result in xHistMinus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xMinus',c='yellow')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xMinus',c='blue')\n","\n","print(f'Does the Newton Method converge? : Success = {successMinus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"y2rWbaCtgbw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PART-(a) with p=0\n","p_ = 0\n","F = lambda x: np.transpose(np.array([F1(x), F2(x,p_)]))\n","plotResidual_partB(p_)\n","\n","#First initial Guess\n","xPlus,successPlus,errorPlus,xHistPlus = newton(F,Jacobian,initialGuessPlus)\n","\n","#separating the vector values into two arrays for x1 and x2\n","resultTop = []\n","resultBottom = []\n","for result in xHistPlus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xPlus',c='r')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xPlus',c='g')\n","\n","print(f'Does the Newton Method converge? : Success = {successPlus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","#Second initial guess\n","xMinus,successMinus,errorMinus,xHistMinus = newton(F,Jacobian,initialGuessMinus)\n","\n","resultTop = []\n","resultBottom = []\n","for result in xHistMinus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xMinus',c='yellow')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xMinus',c='blue')\n","\n","print(f'Does the Newton Method converge? : Success = {successMinus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Gd8RxARjhxTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PART-(a) with p=0.5\n","p_ = 0.5\n","F = lambda x: np.transpose(np.array([F1(x), F2(x,p_)]))\n","plotResidual_partB(p_)\n","\n","#First initial Guess\n","xPlus,successPlus,errorPlus,xHistPlus = newton(F,Jacobian,initialGuessPlus)\n","\n","#separating the vector values into two arrays for x1 and x2\n","resultTop = []\n","resultBottom = []\n","for result in xHistPlus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xPlus',c='r')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xPlus',c='g')\n","\n","print(f'Does the Newton Method converge? : Success = {successPlus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","#Second initial guess\n","xMinus,successMinus,errorMinus,xHistMinus = newton(F,Jacobian,initialGuessMinus)\n","\n","resultTop = []\n","resultBottom = []\n","for result in xHistMinus:\n","  resultTop.append(result[0])\n","  resultBottom.append(result[1]) \n","\n","plt.plot(resultTop,resultBottom,c='black')\n","plt.scatter(resultTop[-1],resultBottom[-1],label='x for xMinus',c='yellow')\n","plt.scatter(resultTop[0],resultBottom[0],label='$x_0$ for xMinus',c='blue')\n","\n","print(f'Does the Newton Method converge? : Success = {successMinus}')\n","print(f'The convergence order is = {convergenceOrder(errorPlus)[0]}')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"J4XGWC3bh6ua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Part C\n","\n","## Richardson iteration\n","def richardson(A,b,x0,alpha0=10,tol=1e-8,maxit=20):\n","  '''\n","  Input\n","  A NxN matrix\n","  b Nx1 right-hand side\n","  x0 (optional) initial guess\n","  alpha0 (optional) static parameter, \n","                if not provided (alpha0=10) >>> dynamic Richardson\n","  tol (optional) desired tolerance\n","  maxIt (optional) maximum number of iterations\n","\n","  Returns:\n","  x approximate solution (last computed)\n","  success true means converged according to error estimator\n","  errEst error estimate per iteration = norm(x(k+1)-x(k))\n","  xHist (optional) array with intermediate solutions\n","  '''\n","\n","\n","  r_k = lambda x: b - np.matmul(A,x)\n","  richardson = lambda x: x + alpha0*r_k(x)\n","\n","  if alpha0 ==10:\n","    alpha0 = lambda x: np.linalg.norm(r_k(x))**2 / np.linalg.norm( np.matmul(np.matmul(np.transpose(r_k(x)),A),r_k(x)))\n","    richardson = lambda x: x + alpha0(x)*r_k(x)\n","    \n","  return fpIterator(richardson,x0,tol,maxit)"],"metadata":{"id":"RGlnNkNbiIYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Discussion C1\n","A = np.vstack((np.array([2,-1]),np.array([-1,2])))\n","b = np.array([1,0])\n","x0 = np.array([0,0])\n","\n","iterations = np.zeros(100)\n","alpha = np.linspace(1/100,1,100)\n","\n","for i in range(100):\n","  xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,(i+1)/100,1e-3,1000)\n","  iterations[i] = len(xHistogram)\n","\n","fig = plt.figure(figsize=(10,10))\n","plt.semilogy(alpha,iterations)\n","makingGraphs(fig,'Number of iterations needed vs value of alpha','alpha','log(iterations)',False)\n","plt.show()\n","\n","\n"],"metadata":{"id":"GDViJGehineI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Discussion C2\n","alphaOpt = 0.5\n","iterations = np.linspace(0,1000,1001)\n","xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,alphaOpt,1e-8,1000)\n","\n","errIterand = np.zeros(len(errorEstimate))\n","error = np.zeros(len(errorEstimate))\n","\n","for i in range(len(errorEstimate)):\n","  errIterand[i] = 1/alphaOpt * np.linalg.norm(np.linalg.inv(A))**2 * np.linalg.norm(errorEstimate[i])\n","  error[i]= np.linalg.norm(xHistogram[i]-xVec)\n","\n","iterations = iterations[:len(errorEstimate)]\n","\n","fig = plt.figure(figsize=(10,10))\n","plt.semilogy(iterations,errorEstimate,label=r'Error Estimate $\\alpha_{opt}$')\n","plt.semilogy(iterations,errIterand,label='Upper bound Iterand error for every iteration k') #added in out of curiosity\n","plt.semilogy(iterations,error, label = r'Actual error for $\\alpha_{opt}$')\n","\n","makingGraphs(fig,'Graph of the error vs the iteration','iteration (k)','log(error)')"],"metadata":{"id":"VMMR2iOYjDlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#alphaOpt\n","alphaOpt = 0.5\n","iterations = np.linspace(0,1000,1001)\n","xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,alphaOpt,1e-8,1000)\n","\n","error = np.zeros(len(errorEstimate))\n","\n","for i in range(len(errorEstimate)):\n","  error[i]= np.linalg.norm(xHistogram[i]-xVec)\n","\n","iterations = iterations[:len(errorEstimate)]\n","\n","fig = plt.figure(figsize=(10,10))\n","plt.semilogy(iterations,errorEstimate,label=r'Error Estimate $\\alpha_{opt}$')\n","plt.semilogy(iterations,error, label = r'Actual error for $\\alpha_{opt}$')\n","\n","#alpha k\n","xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,10,1e-8,1000)\n","\n","error = np.zeros(len(errorEstimate))\n","iterations = np.linspace(0,1000,1001)\n","\n","for i in range(len(errorEstimate)):\n","  error[i]= np.linalg.norm(xHistogram[i]-xVec)\n","\n","iterations = iterations[:len(errorEstimate)]\n","\n","plt.semilogy(iterations,errorEstimate,label=r'Error Estimate $\\alpha_k$')\n","plt.semilogy(iterations,error, label = r'Actual error for $\\alpha_k$')\n","\n","\n","makingGraphs(fig,'Graph of the error vs the iteration','iteration (k)','log(error)')\n","\n","\n"],"metadata":{"id":"cRE178Sli01d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = np.vstack((np.array([1.1,-1]),np.array([-1,1.1])))\n","b = np.array([1,0])\n","x0 = np.array([0,0])\n","\n","iterations = np.zeros(100)\n","alpha = np.linspace(1/100,1,100)\n","\n","for i in range(100):\n","  xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,(i+1)/100,1e-3,1000)\n","  iterations[i] = len(xHistogram)\n","\n","fig = plt.figure(figsize=(10,10))\n","plt.semilogy(alpha,iterations)\n","makingGraphs(fig,'Number of iterations needed vs value of alpha','alpha','log(iterations)',False)\n","plt.show()"],"metadata":{"id":"nbv1VDkKjBXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Discussion C2\n","alphaOpt = 0.9\n","iterations = np.linspace(0,1000,1001)\n","xVec, success, errorEstimate, xHistogram = richardson(A,b,x0,alphaOpt,1e-8,1000)\n","\n","errIterand = np.zeros(len(errorEstimate))\n","error = np.zeros(len(errorEstimate))\n","\n","for i in range(len(errorEstimate)):\n","  errIterand[i] = 1/alphaOpt * np.linalg.norm(np.linalg.inv(A))**2 * np.linalg.norm(errorEstimate[i])\n","  error[i]= np.linalg.norm(xHistogram[i]-xVec)\n","\n","iterations = iterations[:len(errorEstimate)]\n","\n","fig = plt.figure(figsize=(10,10))\n","plt.semilogy(iterations,errorEstimate,label=r'Error Estimate $\\alpha_{opt}$')\n","plt.semilogy(iterations,errIterand,label='Upper bound Iterand error for every iteration k') #added in out of curiosity\n","plt.semilogy(iterations,error, label = r'Actual error for $\\alpha_{opt}$')\n","\n","makingGraphs(fig,'Graph of the error vs the iteration','iteration (k)','log(error)')"],"metadata":{"id":"YzAgjZNijFP7"},"execution_count":null,"outputs":[]}]}